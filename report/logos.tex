% logos.tex
% 

\input opmac
\input lingmac

% Page Layout
\margins/1 letter (1.5, 1.5, 1.5, 1.5)in % letter paper with 1.5" margins

% Fonts
\input kp-fonts % Kepler fonts
%\fontfam[LatinModern]
%\let\bcaps\relax

% Define colors for hyperlinks in PDFs
\def\Rubric{\setcmykcolor{0 0.96 0.89 0.22}}
\def\Tufts{\setcmykcolor{1.0 0.5 0.0 0.0}}
\hyperlinks \Rubric \Tufts

% Rename sectioning commands
\let\title\tit
\let\capter\chap
\let\ssec\secc

%\typosize[9/12]

% "\command" -> \verb|\command|
\activettchar"

% Don't indent subsections in the TOC by so much
\def\tocline#1#2#3#4#5{{\leftskip=#1\iindent \rightskip=2\iindent
	\ifischap\advance\leftskip by\iindent\fi
	\ifnum#1>1 \advance\leftskip by 0.2\iindent\fi
	\toclinehook \noindent\llap{#2\toclink{#3}\enspace}%
		{#2#4}\nobreak\tocdotfill\pglink{#5}\nobreak\hskip-2\iindent\null\par}}

% Title Font
\def\titfont{\typobase\typoscale[\magstep2/\magstep2]\bcaps\bf}
% \def\secfont{\typobase\typoscale[\magstep2/\magstep2]\sans\bf}

% Don't number the first page
\def\folio{\ifnum\pageno<2 \else\number\pageno \fi}

% Don't show black box when text runs into margin
\overfullrule=0pt

\title The Acquisition of Semantic Representations of Anaphora in Complex Expressions

\bigskip
\bigskip
\par\centerline{Jackson Petty}
\centerline{\url{jackson.petty@yale.edu}}
\bigskip
\bigskip
\par\centerline{Summer 2020}
\centerline{Yale University}
\centerline{New Haven, Connecticut}
\bigskip
\bigskip
\par\centerline{Advised by Robert Frank}

\vfill

%\par\centerline{Submitted to the Faculty of the Department of Linguistics}
%\centerline{in partial fulfilment of the requirements for the degree}
%\centerline{of Bachelor of Arts}
%\bigskip

\vfil\eject

\nonum\notoc\sec Contents

\maketoc

\sec Introduction

Well-formed $\multimapdotbothB$ constructions of a natural language have both an observable 
representation, written or spoken words, and a semantic representation, the
meaning behind these utterances. Taken together, the observable and semantic
represenations of a phrase constitue a form-meaning pair. Knowledge of that
language is then the ability to translate between form and meaning, 
interpreting the observable form of an construction and producing new 
observable representations in kind from semantic representations. While to us
the connection between a form and its meaning is intuitive, to machines this
relationship is not innate and must then be taught. 

One challenge in teaching machines to understand language comes from words or
phrases whose meaning is necessarily context-dependent. Simple words, like
proper nouns or verbs, may have meanings which are derivable from the words 
themselves. For instance, consider a simple language consiting of names and
intransitive verbs like the one shown below in~(\nextx).
\pex
	\a{} \denote{Alice}  = the girl named Alice
	\a{} \denote{Bob} = the boy named Bob
	\a{} \denote{thinks} = the act of thinking
\xe
The meanings of sentences in this simple language are nicely composed of the
meanings of the constituent words. For example, in~(\nextx) we see that the
sentence ``Alice thinks'' is understood of one knows the meanings of ``Alice,''
``thinks,'' and the rules about how sentences are formed from constituent 
parts.
\ex{}
	\denote{Alice thinks} $\approx$ \denote{Alice} $+$ \denote{thinks} = the girl named Alice thinks
\xe

Not all words, however, have this nice property that their meanings are 
independent of the words surrounding them. Consider the case of anaphora, where
reflexive pronouns refer to other nouns in a sentence, like in~(\nextx).
\pex
	\a Alice sees herself
	\a Claire sees herself
\xe
The sentences here can't be easily interpreted from the meanings of the 
individual words in the same way that~(\blastx) can. For one, it's clear that
the word ``herself'' doesn't have a meaning independent of ``Alice'' or 
``Claire.'' Furthermore, it's apparent that the meaning of ``herself'' changes
depending on the context. These are in fact the defining properties of 
reflexive pronouns: they must be bound by some independently-defined noun in
order to have meaning. Cases like this pose challenges for machines being
trained to translate forms into meaning.

Another problem facing linguists and computer scientists is trying to
figure out what exactly is happening when machines ``learn'' a task. Defined
extentially, one might think that a machine has learned a task when it can 
successfully complete the problems set before it. That is, if we give a machine
a list of sentences and ask it to translate those sentences into semantic
representations, and it does so correctly, we might think that the machine
has learned how to interpret language.

% Well-formed utterances of natural language have both an  
% \iid observable~representation, known as a \iid form , and a 
% \iid semantic~representation , known as a \iid meaning . The form of an 
% utternance may be the written or auditory manifestation of language that we 
% encounter, while the meaning of an utterance lives only in our minds. To know 
% a language then is to be able to understand the observable form by translating 
% it into a semantic meaning, and to be able to produce new observable forms 
% from new semantic meanings. As humans, the relationship between form-meaning 
% pairs is intuitive to the point of invisibility, but for machines the task of 
% translating between these domains is not innate. Rather, it must be taught.

\sec Background

\ssec Form-Meaning Pairs
\ssec Anaphora and Binding Theory
\ssec Neural Networks and Anaphora

The question of whether or not neural networks can learn the semantic 
representations of anaphora is not new. \cite[Frank2013] posed this problem and
investigated whether or not a real-time RNN network architecture could predict
the meanings of anaphors. Their results found that real-time networks were 
unable to successfully.

\sec The Seq2Seq Network Architecture

\ssec Specifying the Form and Meaning domains

In order to generate training data consisting of form-meaning pairs, I use a
Featural Context-Free Grammar to proceduraly generate sentences and then parse
them into semantic representations using the "nltk" Python library. The Form 
domain is then all possible sentences generated by the grammar, while the
meaning domain is the corresponding collection of semantic representations.

An example, minimal grammar is given below along with the sentences is 
generates and their associated semantic representations.
\begtt
# file: grammar.fcfg
% start S

# Grammatical Rules
S[SEM = <?pred(?subj)>] -> NP[SEM = ?subj] VP[SEM = ?pred]
VP[SEM = <?v(?obj)>] -> V[SEM = ?v] NP[SEM = ?obj]

# Lexical Rules
NP[SEM = <\P.P(alice)>] -> Alice
VP[SEM = <\x.know(x)>] -> knows
\endtt

\sec Experiment: Generalizing anaphora to new antecedents

One of the simplest types of sentences involving reflexive pronouns are 
sentences containg only names, transitive verbs, and reflexive pronouns. An
example of one such sentence is shown below in~(\nextx).
\ex<alice-herself>
	Alice sees herself.
\xe
We define a simple predicate logic where verbs are mapped to predicates, and
subjects and objects are mapped to the arguments to those predicates, as 
in~(\nextx) below.
\ex
	Bob sees Alice $\to$ see(bob, alice)
\xe
In cases where the object of the verb is a reflexive pronoun, like in~(\blastx), the subject and
object arguments to the predicate are simply the subject of the sentence, as
in~(\nextx).
\ex
	Alice sees herself $\to$ see(alice, alice)
\xe
Intransitive sentences are mapped to predicates with a single argument, as 
in~(\nextx).
\ex
	Bob sleeps $\to$ sleep(bob)
\xe

In order to test whether ot not a Seq2Seq model can be trained to generalize 
knowledge of reflexive sentences, we will selectively withhold certain 
sentences generated by the language grammar from the training set and see how
the network performs on these novel cases. In this set of experiments, we will
examine whether the Seq2Seq models can learn to parse `Alice-reflexive' 
sentences like in~\getref{alice-herself}, which are of the form of~(\nextx) below.
\ex
	Alice {\it verbs} herself
\xe

Our Seq2Seq model is made of recurrent encoders and decoders and can 
optionally implement attention. We conduct these tests using SRN, GRU, and LSTM
architectures with No Attention, Additive Attention, and Multiplicative 
attention to gauge the effect that network architecture has on our models' 
abilities to learn to interpret reflexive pronouns and generalize this 
knowledge to new cases. For each combination of recurrent unit and attention,
we train three models separately to see if there is any variance in the 
networks' abilities.

\ex
\hrule width 5in
\smallskip
\table{lrl}{
  {\bf Grammar} \cr
	S & $\to$ & Name VP \cr
	VP & $\to$ & V$_i$ {\tt |} V$_t$ Name {\tt |} V$_t$ Refl \cr \cr
	% {\bf Lexical} \cr
	Name & $\to$ & Alice | Bob | $\dots$ | Zelda \cr
	Refl & $\to$ & himself | herself \cr
	V$_i$ & $\to$ & walks | sleeps | eats | runs | sings | dances | flies | slumbers \cr
	V$_t$ & $\to$ & sees | meets | likes | dislikes | throws | notices | knows
}
\smallskip
\hrule width 5in
\xe
\caption/t Context-free grammar used for Alice-* experiments.

The sentences produced by this grammar are matched to semantic representations
in the following way.

\par\nobreak\medskip
\hrule width 5.5in
\smallskip
\table{lrlllrl}{
	$x$ V$_i$ & $\to$ & {\it verb}($x$) && Alice sleeps & $\to$ & sleep(alice) \cr
	$x$ V$_t$ $y$ & $\to$ & {\it verb}($x$, $y$) && Bob knows Claire & $\to$ & know(bob, claire) \cr
	$x$ V$_t$ Refl & $\to$ & {\it verb}($x$, $x$) && Zelda sees herself & $\to$ & see(zelda, zelda)
}
\smallskip
\hrule width 5.5in
\par\nobreak\medskip
\caption/t Semantic representations of generated sentences for Alice-* experiments.

For example, examples in~(\nextx) show how various types of sentences produced 
by the grammar are parsed into form-meaning pairs.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [intransitive]
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill [transitive]
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill [reflexive]
\xe
Note that in~(\lastx b) the names $P$ and $Q$ need not be distinct. This 
grammar produces sentences like~(\nextx a), where the subject and the object
of the sentence are the same name. As shown by~(\nextx b), these sentences have
identical semantic representations to reflexive sentences.
\pex
	\a Alice sees Alice $\to$ see(Alice, Alice)
	\a Alice sees herself $\to$ see(Alice, Alice)
\xe

\ssec Alice-$\alpha$: Can Alice know herself?

The simplest way to test generalizability of networks to the task of 
interpreting anaphoric expressions is to withold from training all reflexive
sentences with a certain antecedent. In Alice-$\alpha$, we do exactly this,
withholding all sentences of the form
\ex<ex:alice-herself>
	Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\alpha$]
\xe
from the train-test-val split. We then test the networks' abilities to
interpret sentences where ``Alice'' is the antecedent of the reflexive 
pronoun ``herself.''
All other sentences generated by the grammar, including transitive sentences 
where ``Alice'' appears as both the subject and object like in~(\nextx a),
intransitive sentences where ``Alice'' is the subject like in~(\nextx b), 
sentences where ``Alice'' appears only as the object like in~(\nextx c), and
sentences where ``Alice'' appears only as the subject like in~(\nextx d) where 
$P \not= \hbox{``Alice''}$, are included in the training, validation, and 
testing data.
\pex<ex:alice-alpha-p>
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill [present in Alice-$\alpha$]
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
\xe
All sentences generated by the grammar not involving ``Alice,'' such as those
types enumerated in~(\nextx) where $P,Q \not= \hbox{``Alice''}$, are also present in the training, validation, and
testing data.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\alpha$]
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

{\bf Summary of findings:} All networks are capable of successfully 
generalizing to the novel ``Alice'' antecedent with near-perfect accuracy.
We performed 10 runs each for every combination of model architecture ("SRN",
"LSTM", "GRU", and "Transformer") and attention ("None" or "Multiplicative")
and observed that all model structures were able to learn the correct 
generaliztaion with no degredation in performance on the test split or the
generalization split (=near 100\% accuracy averaged across all 10 runs.) This
is notable as it shows how the Seq2Seq model structure is advantages in the
task when compared to the original language modeling approach of \cite[frank2013].
In these experiemnts, the combined encoding and decoding of the input sequences
into the target predicate grammar statements allowed even the simplest networks
("SRN"s with no attention) to complete the task. In the language-modeling
version of the problem, these same networks were unable to successfully 
identify the antecedents of reflexive pronouns in novel constructions.


\ssec Alice-$\beta$: But wait, doesn't Alice know Alice?

To be successful at the task presented in Alice-$\alpha$, networks need to be
capable of lexical generalization in a new input: the token ``Alice'' does not
appear as the antecedent of ``herself'' in training data and the network must
learn to interpret exactly these types of sentences. However, models trained
on the Alice-$\alpha$ do have one potentially useful piece of information at
their disposal: although not exposed to sentences like ``Alice {\em verbs} 
herself'' in training, they are exposed to the closely-related sentences of
the form ``Alice {\em verbs} Alice.'' Though these sentences do not involve
reflexive pronouns, their semantic representations are identical to reflexive
sentences. This means that models in Alice-$\alpha$ may be biased in favor of
producing the correct semantic parses since they already have knowledge that
there exist sentences of the form of~(\getref{ex:alice-alpha-p}a) whose
representation is the target output for novel constructions like~(\getref{ex:alice-herself}).

To account for this possibility and increase the difficulty of the task set 
before the network, the Alice-$\beta$ experiment further withholds sentences
like those in~(\nextx b) below, where ``Alice'' is both the subject and the
object, along with the Alice-reflexive~(\nextx a) sentences withheld in 
Alice-$\alpha$.
\pex<ex:alice-beta-w>
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\beta$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill  ---
\xe
The remaining types of sentences involving ``Alice,'' intransitive sentences 
like those in~(\nextx a), transitive sentences where ``Alice'' is the subject
but not the object like those in~(\nextx b), and transitive sentences where 
``Alice'' is the object but not the subject like those in~(\nextx c), are all
present in the training, validation, and testing datasets.
\pex
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill [present in Alice-$\beta$]
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
\xe
Finally, all sentences involving subjects and objects other than ``Alice'' are 
also present in the training, validation, and testing dataset, as shown below
in~(\nextx) where $P,Q \not= \hbox{Alice}$.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\beta$]
	\a $P$ {\em verbs} $Q$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

To successfully generalize knowledge of~(\blastx) and~(\lastx) to interpret
the Alice-reflexive sentences of~(\getref{ex:alice-beta-w}b), a model must
not only contend with new inputs but must also successfully produce a new 
semantic representation not previously encountered in training.

{\bf Summary of findings:} Similarly to the Alice-$\alpha$ case. we found that
all network structures were able to successfully learn the correct 
generalization across 10 separate runs, achieving near-perfect accuracy in all
cases without regard to architecture or attention. This experiment shows how
the networks can successfully learn to produce novel outputs when presented
with novel inputs (i.e., the network is able to produce the experssion
\ex
	{\it "VERB"}" ( alice , alice )"
\xe
which it has never been trained on when encountering an expression like that
of~\getref{ex:alice-herself}.)



\ssec Alice-$\gamma$: What if nobody knows Alice?

Given the networks' relatively good performance on the harder task of 
Alice-$\beta$, it seems that Seq2Seq models are capable of lexical 
generalization to novel antecedents. To explore the models' limits to this
kind of generalization Alice-$\gamma$ further restricts the kinds of sentences
present in the training data by withholding all sentences like~(\nextx c), 
where ``Alice'' appears as the object, in addition to the~(\nextx ab) 
sentences withheld in Alice-$\beta$.
\pex
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\gamma$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
\xe
Non-reflexive sentences where ``Alice'' serves only as the subject, like the
intransitive sentences of~(\nextx a) or transitive sentences of~(\nextx b) 
where $P \not= \hbox{``Alice''}$, are included in the training, validation, and
testing datasets.
\pex
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill [present in Alice-$\gamma$]
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
\xe
Additionally, sentences of the form of~(\nextx) for $P,Q \not= \hbox{``Alice''}$, where ``Alice'' does not appear at all, are likewise included in the
training, validation, and testing sets.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\gamma$]
	\a $P$ {\em verbs} $Q$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

{\bf SUmmary of findings:} The Alice-$\gamma$ case proved much more difficult
for networks that the -$\alpha$ and -$\beta$ variants. The absence of training
data where ``Alice'' served as tbe object of a transitive verb severely limited
networks' performance on the Alice-reflexive test cases. In general, "SRN"s without attention, "GRU"s without attention, and "LSTM"s without attention
all completely fail at this task, achieving essentially 0\ accuracy on the 
generalization set. "GRU"s with multiplicative attention and "LSTM"s with
multiplicative attention achieve middling performances of accuracies between
10\% and 60\% on the generalization set with most models clustered at the low
end of that range. The only models which proved adept at learning this 
generalization were "SRN"s with multiplicative attention which achieved 
near-perfect accuracy on the generalization set. This finding is significant
for two reasons. First, this experiment observes that attention has a noticable
impact on model performance for this dataset: models without attention achieve
essentially 0\% accuracy, while models with attention do significantly better.
Second, this experiment demonstrates that more complicated model structures
do not necessarily achieve better performance that simpler structures. 
"SRN"s with attention greatly outperform all other model structures, even
once which are more complicated in architecture, even those implementing 
attention. This finding is especially notable when compared to the results of
the Alice-$\epsilon$ and Alice-$\zeta$ runs, where we observe "GRU"s and 
"LSTM"s achieving far-superior performance when compared to other 
architectures, indicating that the experimental domain greatly affects how 
useful attention and model architecture are: it is not simply a matter of 
throwing more complicated structures at more difficult problems.



\ssec Alice-$\gamma*$: What if nobody knows Alice?

In Alice-$\gamma$, we distinguished sentences where ``Alice'' was the subject
of a sentence from those were ``Alice'' was the object of a sentence. In the
input domain, this distinction is well-defined since in these simple sentences
the subject always precedes the verb and the object always follows is. In the
target domain of predicate logic, however, this distinction is muddled. While
there is a distinct linear ordering to the arguments of a verb in the predicate
logic domain, as shown below in~(\nextx), the presence of intransitive 
sentences makes the distinction more complicated.
\pex
	\a Alice {\it verbs} Bob $\to$ {\it verb}(Alice, Bob)
	\a Bob {\it verbs} Alice $\to$ {\it verb}(Bob, Alice)
\xe
In intransitive sentences, there is only a single argument to the verb in the
predicate logic representation. Here, we lose the correspondence bewtween
subject/object positions in the input domain and subject/object positions in 
the target domain. While it is clear to speakers of English that such 
intransitive representations have a subject a no object, there is no structural
reason the network has to not thing that intransitive sentences with only an
object could exist, as shown in~(\nextx).\fnote{Mention unaccusative/passives here?}
\pex
	\a Alice {\it verbs} $\to$ {\it verb}(Alice)
	\a \judge* {\it Verbs} alice $\to$ {\it verb}(Alice)
\xe
In light of this, it is more proper to think about ``first'' and ``second''
position in the predicate logic domain rather than ``subject'' and ``object''
position. 

This clarification raises issue that in Alice-$\gamma$, the network isn't 
baised against interpreting Alice-intransitive sentences as being sentences
where Alice is the object. To clear this up, we run an additional experiment
Alice-$\gamma*$ which additionally witholds all sentences were Alice is the
subject of an intransitive sentence along with the Alice-object sentences
originally held in Alice-$\gamma$.
\pex
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\gamma*$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill ---
\xe
Non-reflexive sentences where ``Alice'' serves only as the subject, like the
intransitive sentences of~(\nextx a) or transitive sentences of~(\nextx b) 
where $P \not= \hbox{``Alice''}$, are included in the training, validation, and
testing datasets.
\pex
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill [present in Alice-$\gamma*$]
\xe
Additionally, sentences of the form of~(\nextx) for $P,Q \not= \hbox{``Alice''}$, where ``Alice'' does not appear at all, are likewise included in the
training, validation, and testing sets.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\gamma*$]
	\a $P$ {\em verbs} $Q$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

{\bf Summary of findings:} In general, we did not observe any major differences
in performancy or learning rate between the Alice-$\gamma$ and Alice-$\gamma*$
experiments. The best accuracy of the "SRN"s with attention was reduced to just below 100\%, and the best accuracies of the "LSTM"s were reduced to around
50\%, but the relative performance of model structures did not change.


\ssec Alice-$\delta$: What if Alice doesn't know anyone?

In the same vein of further restricting the distribution of ``Alice'' in the
training dataset, we also investigate what happens when the network is never
exposed to sentences where ``Alice'' serves as the subject. This means 
withholding both Alice-reflexive~(\nextx a) and Alice-Alice~(\nextx b)
sentences, along with transitive sentences like~(\nextx c) where 
$P \not= \hbox{``Alice''}$.
\pex
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\delta$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill ---
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
\xe


Sentences where ``Alice'' is only the object, like~(\nextx a) for 
$P \not= \hbox{``Alice''}$, and Alice-intransitive sentences like~(\nextx b) are present in the training, validation, and 
testing data, as are all sentences~(\nextx c--e) not involving ``Alice.''
\pex
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill [present in Alice-$\delta$]
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill ---
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill ---
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

{\bf Summary of findings:} Similarly to the Alice-$\gamma$ experiments, we 
observe that the presence of attention has a big influence on the performance
of models. Those without (multiplicative) attention achieved near-0\% accuracy
on the generalization sets, while those with attention achieved far better
results. Variance on the Alice-$\delta$ models was higher than on the -$\gamma$
varieties. Again, the highest performing structure was "SRN"s with attention,
which mostly achieved between 80\% and 100\% accuracy, with one outlier 
attaining only 40\%. "GRU"s with attention did, in general, poorly, with most
models attaining less that 40\% accuracy, though one outlier did achieve 100\%
accuracy. "LSTM"s with attention had much the same middling performance as in
the -$\gamma$ cases.

\ssec Alice-$\delta$*

Similar to the Alice-$\gamma$* case, the question of the impact of intransitive
sentences is worth considering here as well. ``Withholding of all instances of
`Alice' in subject position'' could seem to the models as withholding all 
instances of Alice which correspond to the first argument position in the
predicate domain, and since grammatically subjects of intransitive sentences
serve the same role as subjects of transitive ones it is natural to consider
the affect that Alice-intransitive sentences have on the models' abilities
to generalize. To that end, the Alice-$\gamma$* case further restricts the
training set by with holding the Alice-intransitive sentences of~(\nextx d)
alongisde the Alice-subject sentences of~(\nextx a--c) (again for 
$P \not= \hbox{``Alice''}$).
\pex
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld
	   in Alice-$\delta$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill ---
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill ---
\xe

Sentences where ``Alice'' is only the object, like~(\nextx a) for 
$P \not= \hbox{``Alice''}$, are present in the training, validation, and 
testing data, as are all sentences~(\nextx b--d) not involving ``Alice.''
\pex
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill [present in Alice-$\delta$]
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill ---
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

{\bf Summary of findings:} Distinct from the similarity between the -$\gamma$
and -$\gamma$* runs, witholding intransitive sentences had a large and unusual
impact on the -$\delta$ runs. Performance of models without attention remained
the same (=near 0\% accuracy). Performance of "SRN"s with attention dropped
dramatically, with most models attaining less that 20\% accuracy, excepting one
outlier which achieved 100\% accuracy. "GRU"-attention performance likewise 
flipped, with almost all models scoring above 80\% accuracy on the 
generalization set, excepting one poorly performing model which achieved only 
around 10\% accuracy. In essence, performance of "GRU"s with attention and
"SRN"s with attention flipped. The variance of the "LSTM"s with attention was
increased: most models did slightly worse, achieving around 15--20\% accuracy,
though one model did eek out around 70\% accuracy. These results are surprising
for two reasons: First, it is notable to see how small changes in the dataset
can have such a large impact on model performance. In contrast to the -$\gamma$/-$\gamma$* cases, witholding Alice-intransitive sentneces had a large effect
on how well models were able to generalize. Second, it is very strange that
increasing the difficulty of the training set by witholding more data actually
increased the performance of "GRU"s with attention.


\ssec Alice-$\varepsilon$: Who's Alice and who's Claire?

The previous Alice-* experiments have thus far dealt with the task of lexical 
generalization of a single novel antecedent; we withhold some combinations of
sentences containing the token ``Alice'' and test to see whether or not the
network can learn to interpret sentences where ``Alice'' is the antecedent 
of a reflexive pronoun. While the results Alice-$\beta$ experiment were 
successful  in this regard, there is a concern that the networks may have been 
able to interpret Alice-reflexive sentences in a negative sense: that is, by 
learning the interpretations of all other {\em Person}-reflexive sentences 
through direct example and then ``filling in the blank'' with ``Alice'' when 
confronted with a {\em Person}-reflexive combination which it has not yet been 
taught. When dealing with only a single novel antecedent, this outcome would 
look the same as if the network had truly acquired the generalization 
of~(\nextx).
\ex
	$P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$)
\xe
To determine if these networks are truly capable of the latter inference, we
extend the Alice-$\beta$ experiment by withholding progressively more 
sentences of the form 
\ex
	$P$ {\em verbs} ( $P$ | himself | herself )
\xe
and explore the networks' abilities to generalize knowledge of the 
interpretation of reflexive pronouns to new antecedents.

In withholding more than just a single {\em Person}-reflexive sentence, we must
consider the fact that while speakers of English know that ``himself'' and
``herself'' have the same meaning, distinguished only by $\varphi$-features,
the networks here have no such awareness. There is no {\em a-priori} reason for
a network to associate ``himself'' with ``herself'' aside from their 
similarity in positional distribution in the training data. Because of this,
withholding both ``Alice {\em verbs} ( Alice | herself )'' and 
``Bob {\em verbs} ( Bob | himself )'' may not be qualitatively 
different for the network than withholding only ``Alice {\em verbs} ( Alice | 
herself )'' (with respect to the network's performance at 
interpreting the latter). Therefore, we will begin by removing progressively 
more {\em Person}-reflexive sentences with feminine antecedents.

\bigskip\noindent
{\bf Alice-$\boldmath \varepsilon$-2:} We withhold 
{\em Person}-reflexive sentences where ``Alice'' and ``Claire'' are the 
antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\varepsilon$-2]
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
\xe
The training set then included all {\em Person}-reflexive sentences with 
masculine antecedents and all remaining {\em Person}-reflexive sentences with
feminine antecedents. 

\bigskip\noindent
{\bf Alice-$\boldmath \varepsilon$-3:} We withhold 
{\em Person}-reflexive sentences where ``Alice'', ``Claire'', and ``Eliza'' are the 
antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\varepsilon$-3]
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
	\a Eliza {\em verbs} ( Eliza | herself ) \hfill ---
\xe
The training set then included all {\em Person}-reflexive sentences with 
masculine antecedents and all remaining {\em Person}-reflexive sentences with
feminine antecedents. 

\bigskip\noindent
{\bf Alice-$\boldmath \varepsilon$-6:} We withhold 
{\em Person}-reflexive sentences where ``Alice'', ``Claire'', ``Eliza'', ``Grace'', ``Isla'', and ``Katherine'' are the antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\varepsilon$-6]
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
	\a Eliza {\em verbs} ( Eliza | herself ) \hfill ---
	\a Grace {\em verbs} ( Grace | herself ) \hfill ---
	\a Isla {\em verbs} ( Isla | herself ) \hfill ---
	\a Katherine {\em verbs} ( Katherine | herself ) \hfill ---
\xe
The training set then included all {\em Person}-reflexive sentences with 
masculine antecedents and all remaining {\em Person}-reflexive sentences with
feminine antecedents. 

\bigskip\noindent
{\bf Alice-$\boldmath \varepsilon$-14:} We withhold
{\em Person}-reflexive sentences where ``Alice'', ``Claire'', ``Eliza'', ``Grace'', ``Isla'', and ``Katherine'' are the antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\varepsilon$-14]
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
	\a Eliza {\em verbs} ( Eliza | herself ) \hfill ---
	\a Grace {\em verbs} ( Grace | herself ) \hfill ---
	\a Isla {\em verbs} ( Isla | herself ) \hfill ---
	\a Katherine {\em verbs} ( Katherine | herself ) \hfill ---
	\a Margaret {\em verbs} ( Margaret | herself ) \hfill ---
	\a Neha {\em verbs} ( Neha | herself ) \hfill ---
	\a Patricia {\em verbs} ( Patricia | herself ) \hfill ---
	\a Rachael {\em verbs} ( Rachael | herself ) \hfill ---
	\a Tracy {\em verbs} ( Tracy | herself ) \hfill ---
	\a Ursula {\em verbs} ( Ursula | herself ) \hfill ---
	\a Winnifred {\em verbs} ( Winnifred | herself ) \hfill ---
\xe
The training set then included all {\em Person}-reflexive sentences with 
masculine antecedents and the lone remaining {\em Person}-reflexive sentences 
with a feminine antecedent, ``Yvette'', as shown below in~(\nextx).
\ex
	Yvette {\em verbs} ( Yvette | herself ) \hfill [present in Alice-$\varepsilon$-14]
\xe 

{\bf Summary of findings:} Rather than look at any individual experiment in
the -$\varepsilon$ series, it will be more instructive to consider how models'
performance is affected as the number of (feminine) contexts withheld 
increases.

One notable observation about the entire family of experiments is the realtive
performance of the various model structures. In contrast to the -$\delta$ and
-$\gamma$ experiments, where "SRN"s and "GRU"s with attention attained the
best performance, the -$\varepsilon$ family shows that "SRN"s, in general,
fail at this task even when aided by attention. Conversely, "GRU"s and "LSTM"s
prove quite capable of attaining high accuracy even when all but one feminine
context is withheld during training. 

Of additional interest is the impact that attention has on model performance.
The -$\varepsilon$ family shows that sometimes, attention is {\it not} all
you need, and in fact may prove detrimental. While previous experiments showed
how attention could improve model performance we see here that attention
{\it negatively} impacts the accuracies of "GRU"s; "GRU"s sans attention
outperform those with attention on high-number-withheld training sets.

Finally, we see a case where "LSTM"s have excellent performance when compared
to other architectures. Both attentative and non-attentative variants of 
"LSTM"s have accuracies above 90\%, on par with the non-attentative "GRU"s.

This set of experiments shows how qualitatively different "difficult" probelms
can be for networks when compared to the -$\gamma$/-$\delta$ cases. The 
differences between the effects that structure and attention have on the 
models' success at these tasks is notable since it shows that attaining high
accuracy is not necessarily universal even within the same problem domain;
making the problem ``more difficult'' in one way may impact models differently
than in another way.

From a theoretical standpoint, its worth thinking about what it meants to
generalize to new lexical antecedents in this case. Why is it that the models'
performance on Alice-reflecxive sentences is affected at all by the presence
or absence of reflexive sentences with other antecedents? Obviously this is
the main question we care about in terms of lexical generalization, but 
undestanding what leads to the empirical findings here is worth considering.

One last thing of note in this family of experiments is an examination of 
how quickly models learn the generalization task relative to (1) other structures, (2) the validation set, and (3) the individual names in the
generalization task. We observe that "GRU"s without attention and both
flavors of "LSTM"s learn to solve the generalization task {\it before} they
solve the full validation set, shown by the fact that accuracy on the 
generalization set increases before accuracy on the validation set. We also 
note that models with attention learn the generalization task (and the validation task) within a shorter amount of time that those without; this is
characteristic of learning to generalize ``all at once,'' rather than learning
each separate interpretation of ``herself'' individually.


\ssec Alice-$\zeta$: Who's Alice and who's Claire? part II: electric boogaloo

In Alice-$\zeta$ we repeat the process of Alice-$\epsilon$ but withold 
combinations of masculine and feminine names instead of only witholding 
feminine names. This is to test if there is any underlying connection between
the reflexives ``himself'' and ``herself''. Clearly speakers of English will
undestand that these anaphors differ only in $\varphi$-features, but networks
have no such underlying knowledge or biases. In principle, these could be 
totally separate tokens which do not impact each other in any way.

\bigskip\noindent
{\bf Alice-$\boldmath \zeta$-2:} We withhold 
{\em Person}-reflexive sentences where ``Alice'' and ``Bob'' are the 
antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\zeta$-2]
	\a Bob {\em verbs} ( Bob | himself ) \hfill ---
\xe

\bigskip\noindent
{\bf Alice-$\boldmath \zeta$-4:} We withhold 
{\em Person}-reflexive sentences where ``Alice'', ``Bob'', ``Claire'', and ``Daniel'' are the 
antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\zeta$-4]
	\a Bob {\em verbs} ( Bob | himself ) \hfill ---
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
	\a Daniel {\em verbs} ( Daniel | himself ) \hfill ---
\xe

\bigskip\noindent
{\bf Alice-$\boldmath \zeta$-6:} We withhold 
{\em Person}-reflexive sentences where ``Alice'', ``Bob'', ``Claire'', ``Daniel'', ``Eliza'', and ``Francis'' are the antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\zeta$-6]
	\a Bob {\em verbs} ( Bob | himself ) \hfill ---
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
	\a Daniel {\em verbs} ( Daniel | himself ) \hfill ---
	\a Eliza {\em verbs} ( Eliza | herself ) \hfill ---
	\a Francis {\em verbs} ( Francis | himself ) \hfill ---
\xe

\bigskip\noindent
{\bf Alice-$\boldmath \zeta$-14:} We withhold
{\em Person}-reflexive sentences where ``Alice'', ``Bob'', ``Claire'', ``Daniel'', ``Eliza'', ``Francis'', ``Grace'', ``Henry'', ``Isla'', ``John'', ``Katherine'', ``Lewis'', ``Margaret'', and ``Oscar'' are the antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself ) \hfill [withheld in Alice-$\varepsilon$-14]
	\a Bob {\em verbs} ( Bob | himself ) \hfill ---
	\a Claire {\em verbs} ( Claire | herself ) \hfill ---
	\a Daniel {\em verbs} ( Daniel | himself ) \hfill ---
	\a Eliza {\em verbs} ( Eliza | herself ) \hfill ---
	\a Francis {\em verbs} ( Francis | himself ) \hfill ---
	\a Grace {\em verbs} ( Grace | herself ) \hfill ---
	\a Henry {\em verbs} ( Henry | himself ) \hfill ---
	\a Isla {\em verbs} ( Isla | herself ) \hfill ---
	\a John {\em verbs} ( John | himself ) \hfill ---
	\a Katherine {\em verbs} ( Katherine | herself ) \hfill ---
	\a Lewis {\em verbs} ( Lewis | himself ) \hfill ---
	\a Margaret {\em verbs} ( Margaret | herself ) \hfill ---
	\a Oscar {\em verbs} ( Oscar | himself ) \hfill ---
\xe

\bigskip\noindent
{\bf Alice-$\boldmath \zeta$-24:} We include only the
{\em Person}-reflexive sentences where where ``Yvette'' or ``Xerxes'' serve
as the antecedents, as shown below in~(\nextx).
\pex
	\a Yvette {\em verbs} ( Yvette | herself ) \hfill [present in Alice-$\varepsilon$-24]
	\a Xerxes {\em verbs} ( Xerxes | himself ) \hfill ---
\xe


{\bf Summary of findings:} The -$\zeta$ experiemnts turned out much the same
as the -$\varepsilon$ experiments did, though with a ``delay''. Since each of 
the $\zeta$-$n$ cases had twice as many examples of masculine/feminine 
reflexive contexts as the $\varepsilon$-$n$ cases, the $\zeta$-$n$ had better
performance than the corresponding $\varepsilon$-$n$ runs. In general, 
witholding masculine names doesn't seem to make much of an impact on accuracy
for feminine contexts, and vice-versa.


\sec References

\par\noindent
\bib [Frank2013] R.\ Frank, D.\ Mathis, and W.\ Badecker. {\em The Acquisition of Anaphora by Simple Recurrent Networks}. 18 June 2013. {\em Language Acquisition}.

\bye