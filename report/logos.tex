% logos.tex
% 

\input opmac
\input lingmac

% Page Layout
\margins/1 letter (1.5, 1.5, 1.5, 1.5)in % letter paper with 1.5" margins

% Fonts
\input kp-fonts % Kepler fonts
% \fontfam[TimesRoman]
% \let\bcaps\relax

% Define colors for hyperlinks in PDFs
\def\Auburn{\setcmykcolor{0 0.75 0.75 0.35}}
\hyperlinks \Auburn \Blue

% Rename sectioning commands
\let\title\tit
\let\capter\chap
\let\ssec\secc

\typosize[9/12]

% "\command" -> \verb|\command|
\activettchar"

% Don't indent subsections in the TOC by so much
\def\tocline#1#2#3#4#5{{\leftskip=#1\iindent \rightskip=2\iindent
	\ifischap\advance\leftskip by\iindent\fi
	\ifnum#1>1 \advance\leftskip by 0.2\iindent\fi
	\toclinehook \noindent\llap{#2\toclink{#3}\enspace}%
		{#2#4}\nobreak\tocdotfill\pglink{#5}\nobreak\hskip-2\iindent\null\par}}

% Title Font
\def\titfont{\typobase\typoscale[\magstep2/\magstep2]\bcaps\bf}
% \def\secfont{\typobase\typoscale[\magstep2/\magstep2]\sans\bf}

% Don't number the first page
\def\folio{\ifnum\pageno<2 \else\number\pageno \fi}

% Don't show black box when text runs into margin
\overfullrule=0pt

\title The Acquisition of Semantic Representations of Anaphora in Complex Expressions

\bigskip
\bigskip
\par\centerline{Jackson Petty}
\bigskip
\bigskip
\par\centerline{Summer 2020}
\centerline{Yale University}
\centerline{New Haven, Connecticut}
\bigskip
\bigskip
\par\centerline{Advised by Robert Frank}

\vfill

\par\centerline{Submitted to the Faculty of the Department of Linguistics}
\centerline{in partial fulfilment of the requirements for the degree}
\centerline{of Bachelor of Arts}
\bigskip

\vfil\eject

\nonum\notoc\sec Contents

\maketoc

\sec Introduction

Well-formed constructions of a natural language have both an observable 
representation, written or spoken words, and a semantic representation, the
meaning behind these utterances. Taken together, the observable and semantic
represenations of a phrase constitue a form-meaning pair. Knowledge of that
language is then the ability to translate between form and meaning, 
interpreting the observable form of an construction and producing new 
observable representations in kind from semantic representations. While to us
the connection between a form and its meaning is intuitive, to machines this
relationship is not innate and must then be taught. 

One challenge in teaching machines to understand language comes from words or
phrases whose meaning is necessarily context-dependent. Simple words, like
proper nouns or verbs, may have meanings which are derivable from the words 
themselves. For instance, consider a simple language consiting of names and
intransitive verbs like the one shown below in~(\nextx).
\pex
	\a{} $[[$Alice$]]$ = the girl named Alice
	\a{} $[\![$Bob$]\!]$ = the boy named Bob
	\a{} $[\![$thinks$]\!]$ = the act of thinking
\xe
The meanings of sentences in this simple language are nicely composed of the
meanings of the constituent words. For example, in~(\nextx) we see that the
sentence ``Alice thinks'' is understood of one knows the meanings of ``Alice,''
``thinks,'' and the rules about how sentences are formed from constituent 
parts.
\ex{}
	[[Alice thinks]] $\approx$ [[Alice]] $+$ [[thinks]] = the girl named Alice thinks
\xe

Not all words, however, have this nice property that their meanings are 
independent of the words surrounding them. Consider the case of anaphora, where
reflexive pronouns refer to other nouns in a sentence, like in~(\nextx).
\pex
	\a Alice sees herself
	\a Claire sees herself
\xe
The sentences here can't be easily interpreted from the meanings of the 
individual words in the same way that~(\blastx) can. For one, it's clear that
the word ``herself'' doesn't have a meaning independent of ``Alice'' or 
``Claire.'' Furthermore, it's apparent that the meaning of ``herself'' changes
depending on the context. These are in fact the defining properties of 
reflexive pronouns: they must be bound by some independently-defined noun in
order to have meaning. Cases like this pose challenges for machines being
trained to translate forms into meaning.

Another problem facing linguists and computer scientists is trying to
figure out what exactly is happening when machines ``learn'' a task. Defined
extentially, one might think that a machine has learned a task when it can 
successfully complete the problems set before it. That is, if we give a machine
a list of sentences and ask it to translate those sentences into semantic
representations, and it does so correctly, we might think that the machine
has learned how to interpret language.

% Well-formed utterances of natural language have both an  
% \iid observable~representation, known as a \iid form , and a 
% \iid semantic~representation , known as a \iid meaning . The form of an 
% utternance may be the written or auditory manifestation of language that we 
% encounter, while the meaning of an utterance lives only in our minds. To know 
% a language then is to be able to understand the observable form by translating 
% it into a semantic meaning, and to be able to produce new observable forms 
% from new semantic meanings. As humans, the relationship between form-meaning 
% pairs is intuitive to the point of invisibility, but for machines the task of 
% translating between these domains is not innate. Rather, it must be taught.

\sec Background

\ssec Form-Meaning Pairs
\ssec Anaphora and Binding Theory
\ssec Neural Networks and Anaphora

The question of whether or not neural networks can learn the semantic 
representations of anaphora is not new. \cite[Frank2013] posed this problem and
investigated whether or not a real-time RNN network architecture could predict
the meanings of anaphors. Their results found that real-time networks were 
unable to successfully.

\sec The Seq2Seq Network Architecture

\ssec Specifying the Form and Meaning domains

In order to generate training data consisting of form-meaning pairs, I use a
Featural Context-Free Grammar to proceduraly generate sentences and then parse
them into semantic representations using the "nltk" Python library. The Form 
domain is then all possible sentences generated by the grammar, while the
meaning domain is the corresponding collection of semantic representations.

An example, minimal grammar is given below along with the sentences is 
generates and their associated semantic representations.
\begtt
# file: grammar.fcfg
% start S

# Grammatical Rules
S[SEM = <?pred(?subj)>] -> NP[SEM = ?subj] VP[SEM = ?pred]
VP[SEM = <?v(?obj)>] -> V[SEM = ?v] NP[SEM = ?obj]

# Lexical Rules
NP[SEM = <\P.P(alice)>] -> Alice
VP[SEM = <\x.know(x)>] -> knows
\endtt

\sec Experiment: Generalizing anaphora to new antecedents

One of the simplest types of sentences involving reflexive pronouns are 
sentences containg only names, transitive verbs, and reflexive pronouns. An
example of one such sentence is shown below in~(\nextx).
\ex<alice-herself>
	Alice sees herself.
\xe
We define a simple predicate logic where verbs are mapped to predicates, and
subjects and objects are mapped to the arguments to those predicates, as 
in~(\nextx) below.
\ex
	Bob sees Alice $\to$ see(bob, alice)
\xe
In cases where the object of the verb is a reflexive pronoun, like in~(\blastx), the subject and
object arguments to the predicate are simply the subject of the sentence, as
in~(\nextx).
\ex
	Alice sees herself $\to$ see(alice, alice)
\xe
Intransitive sentences are mapped to predicates with a single argument, as 
in~(\nextx).
\ex
	Bob sleeps $\to$ sleep(bob)
\xe

In order to test whether ot not a Seq2Seq model can be trained to generalize 
knowledge of reflexive sentences, we will selectively withhold certain 
sentences generated by the language grammar from the training set and see how
the network performs on these novel cases. In this set of experiments, we will
examine whether the Seq2Seq models can learn to parse `Alice-reflexive' 
sentences like in~\getref{alice-herself}, which are of the form of~(\nextx) below.
\ex
	Alice {\it verbs} herself
\xe

Our Seq2Seq model is made of recurrent encoders and decoders and can 
optionally implement attention. We conduct these tests using SRN, GRU, and LSTM
architectures with No Attention, Additive Attention, and Multiplicative 
attention to gauge the effect that network architecture has on our models' 
abilities to learn to interpret reflexive pronouns and generalize this 
knowledge to new cases. For each combination of recurrent unit and attention,
we train three models separately to see if there is any variance in the 
networks' abilities.

\ex
\hrule width 5in
\smallskip
\table{lrl}{
  {\bf Grammar} \cr
	S & $\to$ & Name VP \cr
	VP & $\to$ & V$_i$ {\tt |} V$_t$ Name {\tt |} V$_t$ Refl \cr \cr
	% {\bf Lexical} \cr
	Name & $\to$ & Alice | Bob | $\dots$ | Zelda \cr
	Refl & $\to$ & himself | herself \cr
	V$_i$ & $\to$ & walks | sleeps | eats | runs | sings | dances | flies | slumbers \cr
	V$_t$ & $\to$ & sees | meets | likes | dislikes | throws | notices | knows
}
\smallskip
\hrule width 5in
\xe
\caption/t Context-free grammar used for Alice-* experiments.

The sentences produced by this grammar are matched to semantic representations
in the following way.

\par\nobreak\medskip
\hrule width 5.5in
\smallskip
\table{lrlllrl}{
	$x$ V$_i$ & $\to$ & {\it verb}($x$) && Alice sleeps & $\to$ & sleep(alice) \cr
	$x$ V$_t$ $y$ & $\to$ & {\it verb}($x$, $y$) && Bob knows Claire & $\to$ & know(bob, claire) \cr
	$x$ V$_t$ Refl & $\to$ & {\it verb}($x$, $x$) && Zelda sees herself & $\to$ & see(zelda, zelda)
}
\smallskip
\hrule width 5.5in
\par\nobreak\medskip
\caption/t Semantic representations of generated sentences for Alice-* experiments.

For example, examples in~(\nextx) show how various types of sentences produced 
by the grammar are parsed into form-meaning pairs.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [intransitive]
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill [transitive]
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill [reflexive]
\xe
Note that in~(\lastx b) the names $P$ and $Q$ need not be distinct. This 
grammar produces sentences like~(\nextx a), where the subject and the object
of the sentence are the same name. As shown by~(\nextx b), these sentences have
identical semantic representations to reflexive sentences.
\pex
	\a Alice sees Alice $\to$ see(Alice, Alice)
	\a Alice sees herself $\to$ see(Alice, Alice)
\xe

\ssec Alice-$\alpha$: Can Alice know herself?

The Alice-$\alpha$ experiment explores whether a Seq2Seq model can generalize
knowledge of the semantic representations of reflexive sentences to a novel
antecedent. Here, sentences of the form of~(\nextx), where ``Alice'' is the
antecedent of the reflexive pronoun ``herself'' are withheld from the training,
testing, and validation data. 
\ex<ex:alice-herself>
	Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\alpha$]
\xe
All other sentences generated by the grammar, including transitive sentences 
where ``Alice'' appears as both the subject and object like in~(\nextx a),
intransitive sentences where ``Alice'' is the subject like in~(\nextx b), 
sentences where ``Alice'' appears only as the object like in~(\nextx c), and
sentences where ``Alice'' appears only as the subject like in~(\nextx d) where 
$P \not= \hbox{``Alice''}$, are included in the training, validation, and 
testing data.
\pex<ex:alice-alpha-p>
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill [present in Alice-$\alpha$]
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
\xe
All sentences generated by the grammar not involving ``Alice,'' such as those
types enumerated in~(\nextx) where $P,Q \not= \hbox{``Alice''}$, are also present in the training, validation, and
testing data.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\alpha$]
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

The networks' task then is to generalize knowledge of the semantic 
representations of sentences like those in~(\blastx) and~(\lastx) to sentences
like those in~(\getref{ex:alice-herself}).

\ssec Alice-$\beta$: But wait, doesn't Alice know Alice?

To be successful at the task presented in Alice-$\alpha$, networks need to be
capable of lexical generalization in a new input: the token ``Alice'' does not
appear as the antecedent of ``herself'' in training data and the network must
learn to interpret exactly these types of sentences. However, models trained
on the Alice-$\alpha$ do have one potentially useful piece of information at
their disposal: although not exposed to sentences like ``Alice {\em verbs} 
herself'' in training, they are exposed to the closely-related sentences of
the form ``Alice {\em verbs} Alice.'' Though these sentences do not involve
reflexive pronouns, their semantic representations are identical to reflexive
sentences. This means that models in Alice-$\alpha$ may be biased in favor of
producing the correct semantic parses since they already have knowledge that
there exist sentences of the form of~(\getref{ex:alice-alpha-p}a) whose
representation is the target output for novel constructions like~(\getref{ex:alice-herself}).

To account for this possibility and increase the difficulty of the task set 
before the network, the Alice-$\beta$ experiment further withholds sentences
like those in~(\nextx b) below, where ``Alice'' is both the subject and the
object, along with the Alice-reflexive~(\nextx a) sentences withheld in 
Alice-$\alpha$.
\pex<ex:alice-beta-w>
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\beta$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill  ---
\xe
The remaining types of sentences involving ``Alice,'' intransitive sentences 
like those in~(\nextx a), transitive sentences where ``Alice'' is the subject
but not the object like those in~(\nextx b), and transitive sentences where 
``Alice'' is the object but not the subject like those in~(\nextx c), are all
present in the training, validation, and testing datasets.
\pex
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill [present in Alice-$\beta$]
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
\xe
Finally, all sentences involving subjects and objects other than ``Alice'' are 
also present in the training, validation, and testing dataset, as shown below
in~(\nextx) where $P,Q \not= \hbox{Alice}$.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\beta$]
	\a $P$ {\em verbs} $Q$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

To successfully generalize knowledge of~(\blastx) and~(\lastx) to interpret
the Alice-reflexive sentences of~(\getref{ex:alice-beta-w}b), a model must
not only contend with new inputs but must also successfully produce a new 
semantic representation not previously encountered in training.

\ssec Alice-$\gamma$: What if nobody knows Alice?

Given the networks' relatively good performance on the harder task of 
Alice-$\beta$, it seems that Seq2Seq models are capable of lexical 
generalization to novel antecedents. To explore the models' limits to this
kind of generalization Alice-$\gamma$ further restricts the kinds of sentences
present in the training data by withholding all sentences like~(\nextx c), 
where ``Alice'' appears as the object, in addition to the~(\nextx ab) 
sentences withheld in Alice-$\beta$.
\pex
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\gamma$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill ---
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill ---
\xe
Non-reflexive sentences where ``Alice'' serves only as the subject, like the
intransitive sentences of~(\nextx a) or transitive sentences of~(\nextx b) 
where $P \not= \hbox{``Alice''}$, are included in the training, validation, and
testing datasets.
\pex
	\a Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill [present in Alice-$\gamma$]
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
\xe
Additionally, sentences of the form of~(\nextx) for $P,Q \not= \hbox{``Alice''}$, where ``Alice'' does not appear at all, are likewise included in the
training, validation, and testing sets.
\pex
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill [present in Alice-$\gamma$]
	\a $P$ {\em verbs} $Q$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

\ssec Alice-$\delta$: What if Alice doesn't know anyone?

In the same vein of further restricting the distribution of ``Alice'' in the
training dataset, we also investigate what happens when the network is never
exposed to sentences where ``Alice'' serves as the subject. This means 
withholding both Alice-reflexive~(\nextx a) and Alice-Alice~(\nextx b)
sentences, along with transitive sentences like~(\nextx c) where 
$P \not= \hbox{``Alice''}$.
\pex
	\a Alice {\em verbs} herself $\to$ {\em verb}(Alice, Alice) \hfill [withheld in Alice-$\delta$]
	\a Alice {\em verbs} Alice $\to$ {\em verb}(Alice, Alice) \hfill ---
	\a Alice {\em verbs} $P$ $\to$ {\em verb}(Alice, $P$) \hfill ---
\xe
There is the additional question of intransitive sentences where ``Alice'' is
the subject. These~(\nextx) are distinct from the reflexive sentences we are
testing but are the only other case where ``Alice'' is the subject. To explore
how these intransitive sentences affect models' abilities to interpret the
reflexive sentences in question, we run two version of the Alice-$\delta$ 
experiment: one where Alice-intransitive sentences are present in the training,
validation, and testing data, and one where they are withheld.
\ex
	Alice {\em verbs} $\to$ {\em verb}(Alice) \hfill [present in Alice-$\delta$, withheld in Alice-$\delta$*]
\xe

Sentences where ``Alice'' is only the object, like~(\nextx a) for 
$P \not= \hbox{``Alice''}$, are present in the training, validation, and 
testing data, as are all sentences~(\nextx b--d) not involving ``Alice.''
\pex
	\a $P$ {\em verbs} Alice $\to$ {\em verb}($P$, Alice) \hfill [present in Alice-$\delta$]
	\a $P$ {\em verbs} $\to$ {\em verb}($P$) \hfill ---
	\a $P$ {\em verbs} $Q$ $\to$ {\em verb}($P$, $Q$) \hfill ---
	\a $P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$) \hfill ---
\xe

\ssec Alice-$\varepsilon$: Who's Alice and who's Claire?

The previous Alice-* experiments have thus far dealt with the task of lexical 
generalization of a single novel antecedent; we withhold some combinations of
sentences containing the token ``Alice'' and test to see whether or not the
network can learn to interpret sentences where ``Alice'' is the antecedent 
of a reflexive pronoun. While the results Alice-$\beta$ experiment were 
successful  in this regard, there is a concern that the networks may have been 
able to interpret Alice-reflexive sentences in a negative sense: that is, by 
learning the interpretations of all other {\em Person}-reflexive sentences 
through direct example and then ``filling in the blank'' with ``Alice'' when 
confronted with a {\em Person}-reflexive combination which it has not yet been 
taught. When dealing with only a single novel antecedent, this outcome would 
look the same as if the network had truly acquired the generalization 
of~(\nextx).
\ex
	$P$ {\em verbs} ( himself | herself ) $\to$ {\em verb}($P$, $P$)
\xe
To determine if these networks are truly capable of the latter inference, we
extend the Alice-$\beta$ experiment by withholding progressively more 
sentences of the form 
\ex
	$P$ {\em verbs} ( $P$ | himself | herself )
\xe
and explore the networks' abilities to generalize knowledge of the 
interpretation of reflexive pronouns to new antecedents.

In withholding more than just a single {\em Person}-reflexive sentence, we must
consider the fact that while speakers of English know that ``himself'' and
``herself'' have the same meaning, distinguished only by $\varphi$-features,
the networks here have no such awareness. There is no {\em a-priori} reason for
a network to associate ``himself'' with ``herself'' aside from their 
similarity in positional distribution in the training data. Because of this,
withholding both ``Alice {\em verbs} ( Alice | herself )'' and 
``Bob {\em verbs} ( Bob | himself )'' may not be qualitatively 
different for the network than withholding only ``Alice {\em verbs} ( Alice | 
herself )'' (with respect to the network's performance at 
interpreting the latter). Therefore, we will begin by removing progressively 
more {\em Person}-reflexive sentences with feminine antecedents.

\bigskip\noindent
{\bf Alice-$\boldmath \varepsilon$-2:} We begin by withholding 
{\em Person}-reflexive sentences where ``Alice'' and ``Claire'' are the 
antecedents, shown below in~(\nextx).
\pex
	\a Alice {\em verbs} ( Alice | herself )
	\a Claire {\em verbs} ( Claire | herself )
\xe
The training set then included all {\em Person}-reflexive sentences with 
masculine antecedents and all remaining {\em Person}-reflexive sentences with
feminine antecedents. \cite[Frank2014]

\picw=2.5in
\inspic alice-epsilon-2-SRN-SRN-None-model-1.png
\inspic alice-epsilon-2-SRN-SRN-None-model-2.png

\sec References

\par\noindent
\bib [Frank2013] R.\ Frank, D.\ Mathis, and W.\ Badecker. {\em The Acquisition of Anaphora by Simple Recurrent Networks}. 18 June 2013. {\em Language Acquisition}.

\bib [Frank2014] R.\ Frank, D.\ Mathis, and W.\ Badecker. {\em The Acquisition of Anaphora by Simple Recurrent Networks}. 18 June 2013. {\em Language Acquisition}.

\bye